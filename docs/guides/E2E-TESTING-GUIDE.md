# End-to-End Testing Guide

This guide provides step-by-step instructions for validating the Jengu pricing platform with real data.

## ğŸ¯ Overview

E2E testing validates that all components work together correctly:

- âœ… Real data flows from upload â†’ database â†’ analytics â†’ pricing
- âœ… No mock/fake data appears anywhere
- âœ… All integrations work (Supabase, APIs, frontend â†” backend)
- âœ… Performance is acceptable on realistic datasets

## ğŸ“‹ Prerequisites

### 1. Environment Setup

**Backend** (`.env` file):

```env
SUPABASE_URL=your_supabase_url
SUPABASE_ANON_KEY=your_anon_key
SUPABASE_SERVICE_ROLE_KEY=your_service_key
ANTHROPIC_API_KEY=your_anthropic_key (optional for AI insights)
```

**Frontend** (`.env` file):

```env
VITE_SUPABASE_URL=your_supabase_url
VITE_SUPABASE_ANON_KEY=your_anon_key
```

### 2. Test User Account

Create a test user in Supabase:

- Email: `test@jengu.com`
- Password: `test123456`

### 3. Start Servers

Terminal 1 (Backend):

```bash
cd backend
pnpm run dev
```

Terminal 2 (Frontend):

```bash
cd frontend
pnpm run dev
```

## ğŸ§ª Test Data Generation

Generate realistic test data for all scenarios:

```bash
cd backend
npx tsx test/e2e-test-data.ts
```

This creates test CSV files in `backend/test-data/`:

- `test-standard.csv` - 30 days of data (normal scenario)
- `test-insufficient.csv` - 5 rows (edge case)
- `test-noWeather.csv` - Missing weather columns
- `test-large.csv` - 10,000+ rows (performance test)
- `test-oneYear.csv` - 365 days (full year)

## ğŸ”§ Automated API Tests

Run backend integration tests:

```bash
cd backend

# Set test credentials
export TEST_EMAIL=test@jengu.com
export TEST_PASSWORD=test123456

# Run tests
npx tsx test/api-integration.test.ts
```

**Expected output:**

```
ğŸ§ª Starting API Integration Tests

âœ… Authentication - Sign in with email/password
âœ… GET /api/health - Health check endpoint
âœ… GET /api/data/files - Fetch uploaded files
âœ… POST /api/pricing/quote - Request pricing quote with validation
âœ… POST /api/pricing/quote - Reject invalid request (Zod validation)
âœ… POST /api/analytics/revenue-series - Fetch revenue time series
âœ… Database - Verify RLS policies allow user access

ğŸ“Š Test Results Summary
Total: 7 | Passed: 7 | Failed: 0
âœ… All tests passed!
```

## ğŸ“ Manual Testing Scenarios

### Scenario 1: Fresh User Journey (Happy Path)

**Goal**: Validate complete data flow from upload to pricing

**Steps**:

1. **Clear browser data**
   - Open DevTools (F12) â†’ Application tab
   - Clear Local Storage and Session Storage
   - Hard refresh (Ctrl+Shift+R)

2. **Login**
   - Navigate to `http://localhost:5173`
   - Sign in with test user credentials
   - Verify redirect to Dashboard

3. **Upload CSV**
   - Navigate to Data page
   - Upload `test-standard.csv` (30 rows)
   - Verify upload progress bar
   - Verify success message

4. **Verify Dashboard**
   - Navigate to Dashboard
   - Check statistics:
     - Total Records: **30**
     - Average Price: ~**$115** (varies due to randomization)
     - Occupancy Rate: ~**70%**
   - Verify charts display:
     - Revenue over time (monthly bars)
     - Occupancy by day (weekly pattern)
     - Price trends (last 30 days line chart)

5. **Verify Insights Page**
   - Navigate to Insights
   - Check data sections:
     - Weather impact (if weather columns present)
     - Occupancy patterns by day of week
     - Price correlations
   - Click "Generate Analytics"
   - Verify predictions appear (may take 5-10 seconds)
   - If Anthropic API key configured: verify AI insights generate

6. **Generate Pricing**
   - Navigate to Pricing Engine
   - Select property from dropdown
   - Select strategy (Conservative/Balanced/Aggressive)
   - Adjust fine-tuning sliders
   - Click "Generate Pricing"
   - Verify:
     - Recommended price appears
     - Confidence interval shown
     - 14-day forecast table populated
     - Export CSV button works

7. **Database Verification**
   - Open Supabase dashboard
   - Go to Table Editor â†’ `pricing_quotes`
   - Verify latest quote saved:
     - `user_id` matches your test user
     - `property_id` matches uploaded file
     - `recommended_price` is reasonable ($80-$150 range)
     - Timestamps are recent

**Expected Results**:

- âœ… All data displays correctly
- âœ… No fake/mock data visible
- âœ… Analytics predictions reasonable
- âœ… Pricing quote saved to database
- âœ… No console errors

**Common Issues**:

- âŒ "No data available" â†’ Check property selected in dropdown
- âŒ Loading forever â†’ Check backend terminal for errors
- âŒ 401 Unauthorized â†’ Clear localStorage and login again
- âŒ Charts empty â†’ Verify CSV has required columns (date, price, occupied, revenue)

---

### Scenario 2: Insufficient Historical Data

**Goal**: Verify graceful handling of small datasets

**Steps**:

1. Upload `test-insufficient.csv` (5 rows)
2. Navigate to Pricing Engine
3. Generate pricing

**Expected Results**:

- âœ… Warning shown: "Limited historical data - using fallback pricing"
- âœ… Fallback price returned (based on simple average)
- âœ… Application doesn't crash
- âœ… Console shows informative warning

**Pass/Fail**:

- [ ] Warning message displayed
- [ ] Fallback pricing works
- [ ] No crashes or errors

---

### Scenario 3: Missing Weather Columns

**Goal**: Verify graceful degradation when optional data missing

**Steps**:

1. Upload `test-noWeather.csv` (no weather column)
2. Navigate to Insights
3. Check weather-related charts

**Expected Results**:

- âœ… Weather charts show "No weather data available" state
- âœ… Other charts (occupancy, price) still work
- âœ… No errors in console

**Pass/Fail**:

- [ ] Graceful "no data" state shown
- [ ] Other features work normally
- [ ] No JavaScript errors

---

### Scenario 4: Large Dataset Performance

**Goal**: Verify performance on realistic production data

**Steps**:

1. Upload `test-large.csv` (10,000+ rows)
2. Start performance monitoring:
   - Open DevTools â†’ Performance tab
   - Start recording
3. Navigate to Insights
4. Click "Generate Analytics"
5. Stop recording when complete

**Expected Results**:

- âœ… Upload completes in < 5 seconds
- âœ… Analytics processing completes in < 30 seconds
- âœ… Charts render without lag
- âœ… Memory usage stays under 500MB
- âœ… No UI freezing

**Performance Benchmarks**:

- Upload: < 5s for 10k rows
- Analytics: < 30s for statistical analysis
- Chart render: < 2s
- Memory: < 500MB

**Pass/Fail**:

- [ ] Processing time acceptable
- [ ] UI remains responsive
- [ ] Memory usage reasonable

---

## ğŸ› Debugging Tips

### Backend Issues

**Check backend logs:**

```bash
cd backend
pnpm run dev
# Watch terminal output for errors
```

**Common backend errors:**

- `ECONNREFUSED` â†’ Supabase credentials incorrect
- `401 Unauthorized` â†’ JWT token expired or invalid
- `500 Internal Server Error` â†’ Check backend terminal for stack trace

### Frontend Issues

**Check browser console:**

- F12 â†’ Console tab
- Look for red errors
- Check Network tab for failed API requests

**Common frontend errors:**

- `Network Error` â†’ Backend not running
- `undefined is not an object` â†’ Missing data in component
- `CORS error` â†’ Backend CORS misconfigured

### Database Issues

**Verify tables exist:**

```bash
cd backend
node test-db.js
```

**Check RLS policies:**

- Open Supabase dashboard
- Go to Authentication â†’ Policies
- Verify all tables have `SELECT`, `INSERT`, `UPDATE` policies
- Verify policies check `auth.uid() = user_id`

---

## âœ… Final Checklist

Before marking Task 8 complete, verify:

**Data Flow**:

- [ ] CSV upload works end-to-end
- [ ] Data displays in Dashboard with correct statistics
- [ ] Insights page shows real data (not mock)
- [ ] Pricing Engine generates quotes from real historical data
- [ ] Database logs all pricing quotes correctly

**No Mock Data**:

- [ ] Dashboard.tsx uses real Supabase data
- [ ] Insights.tsx uses real analytics data
- [ ] PricingEngine.tsx calls real backend API
- [ ] No `insightsData.ts` file present (deleted in Task 1)

**Validation**:

- [ ] Zod validation works (test with invalid request)
- [ ] Proper error messages shown to user
- [ ] 400 errors returned for bad requests

**Performance**:

- [ ] Large datasets (10k+ rows) handle smoothly
- [ ] No memory leaks observed
- [ ] Charts render in < 2 seconds

**Database**:

- [ ] `pricing_quotes` table populated
- [ ] RLS policies enforce user isolation
- [ ] No SQL errors in backend logs

---

## ğŸ“Š Test Results Template

Use this template to document your test results:

```markdown
# E2E Test Results - [Date]

**Tester**: [Your name]
**Environment**: Local development
**Backend**: http://localhost:3001
**Frontend**: http://localhost:5173

## Automated Tests

- API Integration Tests: âœ… PASS (7/7)

## Manual Tests

### Scenario 1: Fresh User Journey

- CSV Upload: âœ… PASS
- Dashboard Stats: âœ… PASS
- Insights Page: âœ… PASS
- Pricing Engine: âœ… PASS
- Database Logging: âœ… PASS

### Scenario 2: Insufficient Data

- Warning Shown: âœ… PASS
- Fallback Pricing: âœ… PASS

### Scenario 3: Missing Weather

- Graceful Degradation: âœ… PASS

### Scenario 4: Large Dataset

- Upload Performance: âœ… PASS (3.2s for 10k rows)
- Analytics Performance: âœ… PASS (18s processing)
- Memory Usage: âœ… PASS (285MB peak)

## Issues Found

- None

## Conclusion

âœ… All E2E tests passed. Application ready for production use.
```

---

## ğŸš€ Next Steps After Testing

Once all tests pass:

1. **Document results** using template above
2. **Commit changes** to version control
3. **Deploy to staging** environment for QA testing
4. **Run tests again** on staging with real Supabase production instance
5. **User acceptance testing** with beta users

**Production deployment checklist**:

- [ ] All E2E tests pass on staging
- [ ] Performance benchmarks met
- [ ] Error monitoring configured (Sentry, LogRocket)
- [ ] Database backups scheduled
- [ ] SSL certificates configured
- [ ] Environment variables secured

---

**Questions?** Check `docs/developer/ARCHITECTURE.md` for technical details or `CLAUDE.md` for development patterns.
